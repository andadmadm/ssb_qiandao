# -*- coding: utf-8 -*-
"""
å®ç°æœä¹¦å§è®ºå›ç™»å…¥å’Œå‘å¸ƒç©ºé—´åŠ¨æ€
"""
import os
import re
import sys
from copy import copy

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import xml.etree.ElementTree as ET
import time
import logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")

ch = logging.StreamHandler(stream=sys.stdout)
ch.setLevel(logging.INFO)
ch.setFormatter(formatter)
logger.addHandler(ch)

def get_refresh_url(url: str):
    try:
        response = requests.get(url)
        if response.status_code != 403:
            response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        meta_tags = soup.find_all('meta', {'http-equiv': 'refresh'})

        if meta_tags:
            content = meta_tags[0].get('content', '')
            if 'url=' in content:
                redirect_url = content.split('url=')[1].strip()
                print(f"Redirecting to: {redirect_url}")
                return redirect_url
        else:
            print("No meta refresh tag found.")
            return None
    except Exception as e:
        print(f'An unexpected error occurred: {e}')
        return None

def get_url(url: str):
    resp = requests.get(url)
    soup = BeautifulSoup(resp.content, 'html.parser')
    
    links = soup.find_all('a', href=True)
    for link in links:
        if link.text == "æœä¹¦å§":
            return link['href']
    return None
    
def update_clini_file(url: str):
    """
    è¯¥å‡½æ•°è¯»å– `cl.ini` æ–‡ä»¶ï¼ŒæŸ¥æ‰¾ custom_proxy_group=ğŸŒ è‡ªå®šä¹‰ è¡Œï¼Œ
    å¹¶ç”¨æ–°çš„ URL æ ¼å¼æ›¿æ¢ã€‚
    """
    cl_ini_path = 'cl.ini'  # æ ¹æ®éœ€è¦è°ƒæ•´è·¯å¾„

    try:
        # è¯»å– cl.ini æ–‡ä»¶å†…å®¹
        with open(cl_ini_path, 'r', encoding='utf-8') as file:
            cl_ini_content = file.read()

        # æŸ¥æ‰¾å¹¶æ›¿æ¢ custom_proxy_group è¡Œï¼Œåªä¿®æ”¹è¿™ä¸€è¡Œçš„å†…å®¹
        proxy_group_pattern = r'(custom_proxy_group=ğŸŒ è‡ªå®šä¹‰).*'
        new_proxy_group = f'custom_proxy_group=ğŸŒ è‡ªå®šä¹‰`url-test`.*`{url}`300,,50'
        updated_content = re.sub(proxy_group_pattern, new_proxy_group, cl_ini_content)

        # å°†æ›´æ–°åçš„å†…å®¹å†™å› cl.ini
        with open(cl_ini_path, 'w', encoding='utf-8') as file:
            file.write(updated_content)
        
        logger.info(f"æˆåŠŸå°† cl.ini ä¸­çš„è‡ªå®šä¹‰ä»£ç†ç»„æ›´æ–°ä¸º: {new_proxy_group}")

    except FileNotFoundError:
        logger.error(f"æœªæ‰¾åˆ°æ–‡ä»¶ {cl_ini_path}ã€‚")
    except Exception as e:
        logger.error(f"æ›´æ–° cl.ini æ—¶å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == '__main__':
    try:
        # è·å–é‡å®šå‘åçš„ URL
        redirect_url = get_refresh_url('http://' + os.environ.get('kvasd.dpkd.5asfws6fpm.com', 'www.soushu2025.com'))
        time.sleep(2)
        redirect_url2 = get_refresh_url(redirect_url)
        final_url = get_url(redirect_url2)  # å®Œæ•´çš„ URL

        # å°†å®Œæ•´çš„ URL å†™å…¥ ssb_url.txt
        with open('ssb_url.txt', 'w', encoding='utf-8') as file:
            file.write(final_url + '\n')  # å†™å…¥å®Œæ•´çš„ URL
        
        # å°†æ ¼å¼åŒ–åçš„åŸŸåå†™å…¥ ssb_clash.txt
        domain = urlparse(final_url).hostname
        with open('ssb_clash.txt', 'w', encoding='utf-8') as clash_file:
            clash_file.write(f"DOMAIN-SUFFIX,{domain}\n")
        
        logger.info(f'{final_url}')
        
        # æ›´æ–° cl.ini æ–‡ä»¶ä¸­çš„è‡ªå®šä¹‰ä»£ç†ç»„ URL
        update_clini_file(final_url)  # ä½¿ç”¨ final_url æ›¿æ¢ url

        sys.exit(0)
        
    except Exception as e:
        logger.error(f"å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        sys.exit(1)
